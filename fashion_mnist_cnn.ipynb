{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "fashion-mnist-cnn.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyMqA08nGmwn9iI1veWV/pVe",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ashraj98/fashion-mnist-cnn/blob/main/fashion_mnist_cnn.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hOOAsJ3BLhHu"
      },
      "source": [
        "# CNN on Fashion MNIST Dataset\n",
        "### Ashwin Rajgopal"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VgNMxALwL1xq"
      },
      "source": [
        "To start off, import `pytorch` libraries as well as `pyplot` for showing comparison results."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HIEhPd4sJtS8"
      },
      "source": [
        "# import standard PyTorch modules\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "\n",
        "# import torchvision module to handle image manipulation\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "# import pyplot to show comparison results\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T0XhK8YBMAk8"
      },
      "source": [
        "Download the Fashion MNIST dataset and save for future runs."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4PJz9F7RJ-vL"
      },
      "source": [
        "# Use standard FashionMNIST dataset\n",
        "train_set = torchvision.datasets.FashionMNIST(\n",
        "    root = './data/FashionMNIST',\n",
        "    train = True,\n",
        "    download = True,\n",
        "    transform = transforms.Compose([\n",
        "        transforms.ToTensor()                                 \n",
        "    ])\n",
        ")\n",
        "\n",
        "test_set = torchvision.datasets.FashionMNIST(\n",
        "    root = './data/FashionMNIST',\n",
        "    train = False,\n",
        "    download = False,\n",
        "    transform = transforms.Compose([\n",
        "        transforms.ToTensor()                                 \n",
        "    ])\n",
        ")"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eSq2xV5GMIBz"
      },
      "source": [
        "This is the base neural network as specified in the description.\n",
        "* layer 1:\n",
        "  * 2d convolution of 5x5, 8 feature maps (channels out), no padding\n",
        "  * each channel then passes through relu\n",
        "  * each channel then passes through max pooling 2x2, stride 2x2\n",
        "  * output of the layer will be 12x12x8  (12x12 images, 8 channels)\n",
        "* layer 2:\n",
        "  * 2d convolution of 5x5x8, 12 feature maps out, and padding is added to preserve width and height\n",
        "  * each channel then passes through relu\n",
        "  * each channel then passes through max pooling 2x2, stride 2x2\n",
        "  * output of the layer will be 6x6x12 (6x6 image maps, 12 channels)\n",
        "* layer 3:\n",
        "  * fully connected layer, 256 outputs\n",
        "  * outputs passed through relu\n",
        "* layer 4: \n",
        "  * softmax layer with 10 outputs corresponding to classes\n",
        "\n",
        "---\n",
        "\n",
        "To allow this network's second layer kernel size to be adapted for comparison testing, I added an initialization parameter to the network. Since the dimensions needs to be maintained using padding in layer two, I did some calculations to figure out what the padding on each side was and used `nn.ConstantPad2d` to achieve the padding calculated. With this calculation, the next layers work without having to adjust the number of units."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "myo8OEksKUY8"
      },
      "source": [
        "# Build the neural network, expand on top of nn.Module\n",
        "class Network(nn.Module):\n",
        "  def __init__(self, layer2_kernel_size=5):\n",
        "    super().__init__()\n",
        "\n",
        "    self.layer2_kernel_size = layer2_kernel_size\n",
        "    self.layer2_padding_size_start = (layer2_kernel_size - 1) // 2\n",
        "    self.layer2_padding_size_end = self.layer2_padding_size_start if layer2_kernel_size % 2 == 1 else self.layer2_padding_size_start + 1\n",
        "    self.layer2_padding = (\n",
        "        self.layer2_padding_size_start, self.layer2_padding_size_end,\n",
        "        self.layer2_padding_size_start, self.layer2_padding_size_end,\n",
        "    )\n",
        "    # define layers\n",
        "    self.layer1 = nn.Sequential(\n",
        "        nn.Conv2d(in_channels=1, out_channels=8, kernel_size=5),\n",
        "        nn.ReLU(),\n",
        "        nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "    )\n",
        "    self.layer2 = nn.Sequential(\n",
        "        nn.Conv2d(\n",
        "            in_channels=8, out_channels=12, kernel_size=self.layer2_kernel_size,\n",
        "        ),\n",
        "        nn.ConstantPad2d(padding=self.layer2_padding, value=0),\n",
        "        nn.ReLU(),\n",
        "        nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "    )\n",
        "    self.layer3 = nn.Sequential(\n",
        "        nn.Flatten(),\n",
        "        nn.Linear(432, 256),\n",
        "        nn.ReLU()\n",
        "    )\n",
        "    self.layer4 = nn.Sequential(\n",
        "        nn.Linear(256, 10),\n",
        "        nn.Softmax(dim=1)\n",
        "    )\n",
        "\n",
        "  def forward(self, t):\n",
        "    t = self.layer1(t)\n",
        "    t = self.layer2(t)\n",
        "    t = self.layer3(t)\n",
        "    t = self.layer4(t)\n",
        "\n",
        "    return t"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8c00rOyQOETQ"
      },
      "source": [
        "This is function to set the passed in model in eval mode, and run a validation set and return the accuracy."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jaGR9R2BSVy5"
      },
      "source": [
        "def get_accuracy(model, dataloader):\n",
        "  count=0\n",
        "  correct=0\n",
        "\n",
        "  model.eval()\n",
        "  with torch.no_grad():\n",
        "    for batch in dataloader:\n",
        "      images = batch[0]\n",
        "      labels = batch[1]\n",
        "      preds=network(images)\n",
        "      batch_correct=preds.argmax(dim=1).eq(labels).sum().item()\n",
        "      batch_count=len(batch[0])\n",
        "      count+=batch_count\n",
        "      correct+=batch_correct\n",
        "  model.train()\n",
        "  return correct/count"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "coOSUKUSOTdp"
      },
      "source": [
        "This function sets the network to train mode, and trains with the passed in train set, shuffling the dataset every epoch, and using the Adam optimizer with cross entropy loss function. Then, the function runs on the test set, and outputs the accuracy on the test set using the previous function."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5wSi-w3pSavf"
      },
      "source": [
        "def train_network(network, train_set, test_set, lr=0.001, batch_size=1000, epochs=10, shuffle=True):\n",
        "  loader = torch.utils.data.DataLoader(train_set, batch_size=batch_size, shuffle=shuffle)\n",
        "  optimizer = optim.Adam(network.parameters(), lr=lr)\n",
        "\n",
        "  # set the network to training mode\n",
        "  network.train()\n",
        "  for epoch in range(epochs):\n",
        "    for batch in loader:\n",
        "      images = batch[0]\n",
        "      labels = batch[1]\n",
        "      preds = network(images)\n",
        "      loss = F.cross_entropy(preds, labels)\n",
        "\n",
        "      optimizer.zero_grad()\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "  test_loader = torch.utils.data.DataLoader(test_set, batch_size=batch_size)\n",
        "  return get_accuracy(network, test_loader)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dq_rb-SwPB-a"
      },
      "source": [
        "Use the network and train function defined above to train the network using different kernel sizes."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Twz8YNEkb-w2"
      },
      "source": [
        "kernel_sizes = [3, 4, 5, 6, 7, 8]\n",
        "accuracies = []\n",
        "for ks in kernel_sizes:\n",
        "  network = Network(layer2_kernel_size=ks)\n",
        "  acc = train_network(network=network, train_set=train_set, test_set=test_set)\n",
        "  accuracies.append(acc)\n",
        "fig = plt.figure()\n",
        "ax = plt.axes()\n",
        "ax.plot(kernel_sizes, accuracies, '-b')\n",
        "plt.xlabel('Kernel Size')\n",
        "plt.ylabel('Accuracy')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0-KvVJTaPOG8"
      },
      "source": [
        "From the above graphic, it can be seen that the network performs better with a smaller kernel size than a larger one.\n",
        "\n",
        "---\n",
        "\n",
        "To optimize the network, I first updated the convolution layers to use 3x3 kernels. To keep the convolution outputs easily poolable using a 2x2 max pool kernel, I used 3x3 padding on the first layer convolution so the output is 32x32, so the max pool result is 16x16. Then in the second layer, I can again use a 3x3 convolution layer, and use a 1x1 padding to maintain 16x16. Then the max pool will reduce the size to 8x8.\n",
        "\n",
        "I then optimized the model by increasing the number of channels to 15 in the first layer and 30 in the second layer. Increasing the channels increases number of inputs in the dense layer, so to account for that, I added another dense layer so the number of outputs wouldn't shrink so quickly from 1920 to 10.\n",
        "\n",
        "After these optimizations, I was around 87% accuracy using the default training parameters from the `train_network` function defined above. To improve this, I added dropout to every layer, using `nn.Dropout2d` on layers before `nn.Flatten`, and ``nn.Dropout`` afterwards. I started by using 50% dropout, but this was hurting performance a little bit, so I reduced dropout to 25%, which improved the results.\n",
        "\n",
        "Finally, I added batch normalization after every ReLU and max pool, to help regularize values from convolutions and max pooling."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rDkn3o3YDqA8"
      },
      "source": [
        "class BetterNetwork(nn.Module):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "\n",
        "    # define layers\n",
        "    # 28x28x1\n",
        "    self.layer1 = nn.Sequential(\n",
        "        nn.Conv2d(in_channels=1, out_channels=15, kernel_size=3, padding=3),\n",
        "        nn.ReLU(),\n",
        "        nn.BatchNorm2d(num_features=15),\n",
        "        nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "        nn.BatchNorm2d(num_features=15),\n",
        "        nn.Dropout2d(.25),\n",
        "    )\n",
        "    # 16x16x15\n",
        "    self.layer2 = nn.Sequential(\n",
        "        nn.Conv2d(in_channels=15, out_channels=30, kernel_size=3, padding=1),\n",
        "        nn.ReLU(),\n",
        "        nn.BatchNorm2d(num_features=30),\n",
        "        nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "        nn.BatchNorm2d(num_features=30),\n",
        "        nn.Dropout2d(.25),\n",
        "    )\n",
        "    # 8x8x30\n",
        "    self.layer3 = nn.Sequential(\n",
        "        nn.Flatten(),\n",
        "        nn.Linear(1920, 512),\n",
        "        nn.ReLU(),\n",
        "        nn.BatchNorm1d(num_features=512),\n",
        "        nn.Dropout(.25)\n",
        "    )\n",
        "    # 512x1\n",
        "    self.layer4 = nn.Sequential(\n",
        "        nn.Linear(512, 256),\n",
        "        nn.ReLU(),\n",
        "        nn.BatchNorm1d(num_features=256),\n",
        "        nn.Dropout(.25)\n",
        "    )\n",
        "    # 256x1\n",
        "    self.layer5 = nn.Sequential(\n",
        "        nn.Linear(256, 10),\n",
        "        nn.Softmax(dim=1)\n",
        "    )\n",
        "\n",
        "  def forward(self, t):\n",
        "    t = self.layer1(t)\n",
        "    t = self.layer2(t)\n",
        "    t = self.layer3(t)\n",
        "    t = self.layer4(t)\n",
        "    t = self.layer5(t)\n",
        "\n",
        "    return t"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FQUA7bTbM2Ut"
      },
      "source": [
        "network = BetterNetwork()\n",
        "acc = train_network(network=network, train_set=train_set, test_set=test_set)\n",
        "print(acc)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V6yMM_dcTWlB"
      },
      "source": [
        "After running this network using default training parameters, I was able to get 90.23% accuracy."
      ]
    }
  ]
}